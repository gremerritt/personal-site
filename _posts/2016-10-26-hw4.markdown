---
layout: post
title:  "Homework 4"
date:   2016-10-26 13:20:10 -0400
categories: cs585
---

### Goals

The goal of this assignment was to design and implement algorithms that can segment and track moving objects in a video. We were given a video of eels swimming in a tank, and our goal was to be able to scan the video and determine the segment the eels such that we could determine where in the video the eels appear.

In particular, the goals were:
  1. Find the boundary box of the two tanks. Note that this boundary box should be determined such that your code can run with the location of the camera moved.
  2. Automatically find the portions of the video with animal action.
  3. Segment the eels and crabs from the background using the techniques you have already learned.
  4. Track the movement of the eels and the crabs i.e., identify the same part of eels (its tail, head and center of the body) and the center of mass of the crabs from frame to frame.
  5. Analyze the movement behavior of eels. Eels make wave-like movements with their bodies. Design an algorithm that describes this movement, in particular, the frequency of undulations.
  6. Your final output should show the eels and crabs being segmented and tracked in the original video by showing the parts of their body being tracked. In addition to that, you should save the times when they are in the open feeding area (with white background) and the estimated undulation frequency in a CSV file.
  7. Your code should be well-commented.

### Approach

To process the video, I simply read each frame of the video in a loop. On each frame of the video, I would call my `process_frame()` function. In order to get a good segmentation of the eels, I first found a frame of the video which contained NO eels (i.e. it looked in the video like it was just an empty tank.) This frame, which I called a 'mask', is defined by the `REFERENCE_MIN` and `REFERENCE_SEC` variables. I first go the this time in the video, and call my processing function on that frame. Since this is the first time the function is called, it handles the frame differently than it will later.

After converting the image to grayscale, it first finds the bounding box of the tank. To do this I did a fairly straightforward thresholding, and determined the bounding box based on a histogram of the white pixels. The bounding box is only found during this initial processing, and this same bounding box is used on future frames.

The processing of the image is also fairly simple. I first do an adaptive threshold to pick out the object in the bounding box. I then blur that image, then do another thresholding operation on the image - that's it! During the first call of this function (the one that doesn't have any eels), we set the `mask` Mat to our processed frame.

Then, I set the video back to the whichever starting place is defined in the program (determined by the `START_MIN` and `START_SEC` variables.) I used these so I could go straight to the interesting segments of the video and they could easily be set to the beginning of the video. Each time the frame is processed, the same processing outlined above is done to the frame (using the same bounding box found in the initial call.) The `mask` is then essentially XOR'ed with the processed frame (with my `subtract()` function):

{% highlight c++ %}
for(int i = upper_left.y; i < lower_right.y; i++) {
  for(int j = upper_left.x; j < lower_right.x; j++) {
    uchar val = (src.at<uchar>(i,j) == 255 && mask.at<uchar>(i,j) == 255) ? 0 : src.at<uchar>(i,j);
    src.at<uchar>(i,j) = val;
  }
}
{% endhighlight %}

Using this method, I'm able to pick out differences between the masking frame and the current frame. With this resulting frame, I'm able to pick out the eels! To find them, I used my stack-based connected component algorithm, which I outlined in the last assignment. Since the eels are relatively big, and the errors in my masking method are small, I can simply ignore small objects picked out by the connected component algorithm, and be fairly sure that the eels will be picked up correctly. I also calculate the boundary of the object, which I use just for display purposes.

Next, I determine the skeleton of each of these objects that I picked out. I used the skeleton algorithm we discussed in class, where a pixel belongs to the skeleton if its minimum distance to the background is greater than or equal to the those of it's neighbors. I then pick out 3 points on this skeleton which are (or are attempted to be) evenly spaced - this is done in my `getPointsOnSkeleton()` function. If the object is skinny and tall I sort the skeleton points vertically, and if the object short and wide I sort the points horizontally. I then pick my three points to be the first point, middle point, and last point. I also built the function so that I could use more points (I tried out 4 and 5) but the results were really best with just 3 points. These three points are displayed on the frame, so you can get and idea of how well this methods works.

Finally, I calculate the curvature of the eel. To do this, I used the last method described in the handout given in class:

|<sup>u<sub>i</sub></sup> / <sub>|u<sub>i</sub>|</sub> - <sup>u<sub>i+1</sub></sup> / <sub>|u<sub>i+1</sub>|</sub>|<sup>2</sup>

If more than 3 points on the skeleton were used, I would take the calculate all the curvature values and then take the average.

Lastly I calculated the change in curvature from the last frame. To do this I used a fairly simple matching algorithm to determine which eel in the current frame was matched to an eel in the previous frame. Each time a frame is processed, I keep track of the eels in that frame, including where that eels is (its center of mass) and it's curvature. Then in the current frame, I have another list of the eels in the frame, and their locations and curvatures. The objects that are closest together and said to be the same object. For example, if there were two eels in the last frame, and three in the current frame, then the eel in the current frame with the closest distance to an eel in the last frame are matched. These two eels (one in the current frame, one in the last frame) are then taken out of consideration. The process is done again. Since there is still an eel in the current frame with no match in the last frame, we just ignore it and essentially assume it 'just appeared'.

The difference in curvature (i.e. rate of change per frame) is then written to a file, along with the timestamp in the video. Thus, we could run the program over the entire video, and then we could check the file to see the locations in the video where there were eels, and how much their curvatures were changing. Hopefully, this is a good measure of how much distress that eel is in!

A GIF of my results can be seen below, along with a sample of the file produced.

![eels.gif](../../../../_images/cs585/hw4/eels.gif)


    time,change_in_curvature1,change_in_curvature2,...
    12:28:714,0.061457
    12:28:781,0.181825
    12:28:814,0.066664
    12:28:848,0.994980
    12:28:881,0.034784
    12:28:948,0.132038
    12:28:981,0.023979
    12:29:14,0.431118
    12:29:48,0.431491
    12:29:115,0.447450
